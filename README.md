# aiSFX

![Picture](./imgs/ucs_black_badge.png)

 Representation Learning for the Automatic Indexing of Sound Effects Libraries (ISMIR 2022): Deep audio embeddings pre-trained on UCS & Non-UCS-compliant datasets.

This work was inspired by the creation of the [Universal Category System (UCS)](https://universalcategorysystem.com), an industry-proposed public domain initiative initialized by [Tim Nielsen](https://www.imdb.com/name/nm0631004), [Justin Drury](https://twitter.com/jaydee2190), [Kai Paquin](https://www.imdb.com/name/nm5226068), and others. First launching in the fall of 2020, UCS offers a standardized framework for sound effects library metadata designed by and for sound designers and editors.

# How To Use

Please refer to this package's [documentation](https://aisfx.readthedocs.io/en/latest/) for [**Installation Instructions**](https://aisfx.readthedocs.io/en/latest/installation.html) and [**Tutorials**](https://aisfx.readthedocs.io/en/latest/notebooks/tutorial.html) of how to extract embeddings.

# [Visualizations of UCS Classes](https://public.tableau.com/views/X-Sequential-CE_UCS/TRAIN-ProSoundEffectsPSE?:language=en-US&:display_count=n&:origin=viz_share_link)

Click the above to visualize coarse-level "Category" UCS classes in Pro Sound Effects (PSE), Soundly (SDLY), and UCS Mixed (UMIX).

# Cite This Work

Please cite the paper below if you use it in your work.

This paper has been accepted at the 23rd International Society for Music Information Retrieval Conference (ISMIR) in Bengaluru, India (December 04-08, 2022). To cite our work, please refer to the following.

[1] [Representation Learning for the Automatic Indexing of Sound Effects Libraries](https://zenodo.org/record/7316800#.ZAU5pezMLzd)

      @inproceedings{ismir_aisfx,
        title={Representation Learning for the Automatic Indexing of Sound Effects Libraries},
        author={Ma, Alison Bernice and Lerch, Alexander},
        booktitle={Proceedings of the 23rd International Society for Music Information Retrieval Conference (ISMIR)},
        year={2022},
        pages={866--875}
      }
      
     
# Acknowledgements
We would like to thank those who provided the data required to conduct this research as well as those who took the time to share their insights and software licenses for tools regarding sound search, query, and retrieval.

[Universal Category System (UCS)](https://universalcategorysystem.com) • [Alex Lane](https://www.alex-lane.com) • [All You Can Eat Audio](https://allyoucaneataudio.com) • [Articulated Sounds](https://articulatedsounds.com) • [Audio Shade](https://audioshade.com) • [aXLSound](https://axlsound.com) • [Big Sound Bank](https://bigsoundbank.com) • [BaseHead](https://baseheadinc.com) • [Bonson](https://www.bonson.ca) • [BOOM Library](https://www.boomlibrary.com) • [Frick & Traa](https://www.frickandtraa.com) • [Hzandbits](https://www.hzandbits.com) • [InspectorJ](https://www.jshaw.co.uk/inspectorj) • [Kai Paquin](https://www.imdb.com/name/nm5226068) • [KEDR Audio](https://www.asoundeffect.com/sounddesigner/kedr-audio) • [Krotos Audio](https://www.krotosaudio.com) • [Nikola Simikic](https://www.imdb.com/name/nm4851270) • [Penguin Grenade](https://www.paulstoughton.com) • [Pro Sound Effects](https://www.prosoundeffects.com) • [Rick Allen Creative](https://www.rickallencreative.com) • [Sononym](https://www.sononym.net/) • [Sound Ideas](https://www.sound-ideas.com) • [Soundly](https://www.getsoundly.com) • [Soundminer](https://store.soundminer.com) • [Storyblocks](https://www.storyblocks.com) • [Tim Nielsen](https://www.imdb.com/name/nm0631004) • [Thomas Rex Beverly](https://thomasrexbeverly.com) • [ZapSplat](https://www.zapsplat.com)

# License: Pre-trained Model & Paper

This pre-trained model and paper [1] is made available under a Creative Commons Attribution 4.0 International License (CC BY 4.0).
